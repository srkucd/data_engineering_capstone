{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 0: Preparation and import data from s3\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (1.14.20)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.20 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from boto3) (1.17.20)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.20->boto3) (1.22)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.20->boto3) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.20->boto3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.20->boto3) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: s3fs in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (0.4.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from s3fs) (0.7.4)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from s3fs) (1.17.20)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.22)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.14)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyspark in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from pyspark) (0.10.9)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: cqlsh in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (5.0.4)\n",
      "Requirement already satisfied: cql in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from cqlsh) (1.4.0)\n",
      "Requirement already satisfied: cassandra-driver in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from cqlsh) (3.24.0)\n",
      "Requirement already satisfied: thrift in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from cql->cqlsh) (0.13.0)\n",
      "Requirement already satisfied: six>=1.9 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from cassandra-driver->cqlsh) (1.11.0)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from cassandra-driver->cqlsh) (0.2.1.post1)\n",
      "Requirement already satisfied: click in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from geomet<0.3,>=0.1->cassandra-driver->cqlsh) (6.7)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: findspark in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (1.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /Users/ishirunkang/anaconda3/lib/python3.6/site-packages (from pyarrow) (1.19.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Before we continue, we need to install related python package.\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install boto3\n",
    "!{sys.executable} -m pip install s3fs\n",
    "!{sys.executable} -m pip install pyspark\n",
    "!{sys.executable} -m pip install cqlsh\n",
    "!{sys.executable} -m pip install findspark\n",
    "!{sys.executable} -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import uuid\n",
    "from pyspark.sql import types as T\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('iam.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS_CREDS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_CREDS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "client=boto3.client('s3')\n",
    "\n",
    "\n",
    "# Set spark environments\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope the Project and Gather Data\n",
    "\n",
    "#### Project description:\n",
    "\n",
    "This project will be separate to multiple parts, and all four dataset will be used. \n",
    "\n",
    "Before we talk about the details, we need to know the characteristics of relational DB and non-relational DB.\n",
    "\n",
    "For relational DB, its characteristics is low redundancy and high completeness, which means it is very suitable for small or medium size data, and the database does not change so much. In our case, we should store temperature, airport code and US cities demographic data into a relational database that meets 3NF because it does not always change so much and the volume of data is not that large.\n",
    "\n",
    "The final solution will work as a a database management system. When user input the time or time period and the column they interested in (e.g., visa type), the system will return related data as a dataframe. For example, a user needs to know where are the busiest airport for investment visa holder(E-1 visa) in 2016 and its basic information such as temperature, and the status of the city such as population(age, majority race, etc.), or when is the peak-time for international student come to the United States and where are they come from.\n",
    "\n",
    "* Data will be imported from Amazon S3\n",
    "* Relational DB will be implement on AWS Redshift\n",
    "* Non-Relationalship DB will be implement on Amazon Keyspace, and data backup will be stored at S3 as parquet format.\n",
    "* Data cleaning and ETL process will be implement on Amazon EMR with Spark\n",
    "\n",
    "#### The dataset is going to use in this project are:\n",
    "\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. https://travel.trade.gov/research/reports/i94/historical/2016.html is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "* World Temperature Data: This dataset came from Kaggle. You can read more about it here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data.\n",
    "* U.S. City Demographic Data: This data comes from OpenSoft. You can read more about it here: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/.\n",
    "* Airport Code Table: This is a simple table of airport codes and corresponding cities. It comes from here:https://datahub.io/core/airport-codes#data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    ".config(\"spark.driver.memory\", \"15g\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94 = pd.read_sas('i94_jan16_sub.sas7bdat', 'sas7bdat',encoding=\"ISO-8859-1\").drop_duplicates()\n",
    "i94['id_'] = pd.Series([str(uuid.uuid1()) for each in range(len(i94))])\n",
    "i94['arrival_date'] = pd.to_timedelta(i94['arrdate'],unit='D') + pd.Timestamp('1960-1-1')\n",
    "i94=spark.createDataFrame(i94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_processor(names):\n",
    "    origin=open('mappings/{}.txt'.format(names),'r')\n",
    "    code=[]\n",
    "    name=[]\n",
    "    for each in origin:\n",
    "        line=\" \".join(each.split())\n",
    "        try:\n",
    "            code.append(int(line[:line.index('=')]))\n",
    "        except:\n",
    "            code.append(line[1:line.index('=')-1])\n",
    "        name.append(line[line.index('=')+2:-1])\n",
    "    origin.close()\n",
    "    col_code=names+'_code'\n",
    "    col_name=names+'_name'\n",
    "    df=pd.DataFrame(list(zip(code,name)),columns=[col_code,col_name])\n",
    "    df=spark.createDataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "country=mapping_processor('country')\n",
    "mode=mapping_processor('mode')\n",
    "port=mapping_processor('port')\n",
    "us_states=mapping_processor('us_states')\n",
    "visacode=mapping_processor('visacode')\n",
    "\n",
    "country.createOrReplaceTempView('country')\n",
    "mode.createOrReplaceTempView('mode')\n",
    "port.createOrReplaceTempView('port')\n",
    "us_states.createOrReplaceTempView('us_states')\n",
    "visacode.createOrReplaceTempView('visacode')\n",
    "i94.createOrReplaceTempView('i94')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"SELECT i94yr AS year,\n",
    "              i94mon AS month,\n",
    "              i94cit AS citizenship,\n",
    "              i94res AS resident,\n",
    "              i94port AS port,\n",
    "              arrival_date,\n",
    "              i94mode AS mode,\n",
    "              i94addr AS us_state,\n",
    "              depdate AS depart_date,\n",
    "              i94bir AS age,\n",
    "              i94visa visa_category,\n",
    "              dtadfile AS date_added,\n",
    "              visapost AS visa_issued_by,\n",
    "              occup AS occupation,\n",
    "              entdepa AS arrival_flag,\n",
    "              entdepd AS depart_flag,\n",
    "              entdepu AS update_flag,\n",
    "              matflag AS match_arrival_depart_flag,\n",
    "              biryear AS birth_year,\n",
    "              dtaddto AS allowed_date,\n",
    "              gender,\n",
    "              insnum AS ins_number,\n",
    "              airline,\n",
    "              admnum AS admission_number,\n",
    "              fltno AS flight_no,\n",
    "              visatype,\n",
    "              id_\n",
    "              FROM i94;\n",
    "       \"\"\"\n",
    "i94_df=spark.sql(sql)\n",
    "i94_df.createOrReplaceTempView('i94')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|mode|count(mode)|\n",
      "+----+-----------+\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp=spark.sql(\"SELECT mode, COUNT(mode) FROM i94 WHERE NOT (mode = 'air') GROUP BY mode\")\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_land_temperature_url = 's3://srk-data-eng-capstone/GlobalLandTemperaturesByCity.csv'\n",
    "airport_codes_url = 's3://srk-data-eng-capstone/airport-codes_csv.csv'\n",
    "us_city_demographics_url = 's3://srk-data-eng-capstone/us-cities-demographics.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global land temperature view preparation\n",
    "global_land_temperature = pd.read_csv(global_land_temperature_url)\n",
    "global_land_temperature['id_'] = pd.Series([str(uuid.uuid1()) for each in range(len(global_land_temperature))])\n",
    "global_land_temperature = spark.createDataFrame(global_land_temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------+---------+\n",
      "|             city|            summer|latitude|longitude|\n",
      "+-----------------+------------------+--------+---------+\n",
      "|         Glendale|            28.416|  32.95N|  112.02W|\n",
      "|          Abilene|            21.666|  32.95N|  100.53W|\n",
      "|      Albuquerque|            21.666|  34.56N|  107.03W|\n",
      "|       Huntsville|            26.982|  34.56N|   85.62W|\n",
      "|       Huntsville|27.593000000000004|  34.56N|   85.62W|\n",
      "|         El Monte|            27.813|  34.56N|  118.70W|\n",
      "|       Alexandria|23.555999999999997|  39.38N|   76.99W|\n",
      "|Lexington Fayette|            31.184|  37.78N|   85.42W|\n",
      "|            Tampa|            27.427|  28.13N|   82.73W|\n",
      "|         Torrance| 26.63000000000001|  34.56N|  118.70W|\n",
      "|           Joliet|            28.566|  40.99N|   87.34W|\n",
      "|            Salem|32.300000000000004|  44.20N|  122.98W|\n",
      "|       Des Moines|32.300000000000004|  40.99N|   93.73W|\n",
      "|       Des Moines|             27.68|  40.99N|   93.73W|\n",
      "|       Evansville|            27.491|  37.78N|   87.46W|\n",
      "|          Fremont|            25.397|  37.78N|  122.03W|\n",
      "|       Long Beach|            18.984|  32.95N|  117.77W|\n",
      "|       Long Beach|29.195999999999998|  32.95N|  117.77W|\n",
      "|         Beaumont|28.084000000000003|  29.74N|   94.15W|\n",
      "|    Thousand Oaks|            23.483|  34.56N|  118.70W|\n",
      "+-----------------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_land_temperature.createOrReplaceTempView('glt')\n",
    "# sql=\"\"\"SELECT DISTINCT city, AverageTemperature AS winter_avg FROM glt WHERE dt='2012-01-01'\"\"\"\n",
    "# winter=spark.sql(sql)\n",
    "# winter.createOrReplaceTempView('winter')\n",
    "# sql=\"\"\"SELECT DISTINCT city, AverageTemperature AS summer_avg FROM glt WHERE dt='2012-07-01'\"\"\"\n",
    "# summer=spark.sql(sql)\n",
    "# summer.createOrReplaceTempView('summer')\n",
    "\n",
    "sql=\"\"\"\"\"\"\n",
    "temp=spark.sql(sql)\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+--------------------+\n",
      "|       city|        dt|      country|              winter|\n",
      "+-----------+----------+-------------+--------------------+\n",
      "|    Abilene|2012-01-01|United States|               7.996|\n",
      "|      Akron|2012-01-01|United States|-0.34399999999999986|\n",
      "|Albuquerque|2012-01-01|United States|                1.84|\n",
      "| Alexandria|2012-01-01|United States|               2.382|\n",
      "|  Allentown|2012-01-01|United States|-0.04099999999999...|\n",
      "|   Amarillo|2012-01-01|United States|               6.534|\n",
      "|    Anaheim|2012-01-01|United States|              15.113|\n",
      "|  Anchorage|2012-01-01|United States|             -22.628|\n",
      "|  Ann Arbor|2012-01-01|United States|               -1.44|\n",
      "|    Antioch|2012-01-01|United States|               9.993|\n",
      "|  Arlington|2012-01-01|United States|               9.359|\n",
      "|  Arlington|2012-01-01|United States|               2.382|\n",
      "|     Arvada|2012-01-01|United States|              -5.402|\n",
      "|    Atlanta|2012-01-01|United States|  6.7860000000000005|\n",
      "|     Aurora|2012-01-01|United States|  0.7640000000000002|\n",
      "|     Aurora|2012-01-01|United States| -1.1059999999999999|\n",
      "|     Austin|2012-01-01|United States|              12.671|\n",
      "|Bakersfield|2012-01-01|United States|   8.607999999999999|\n",
      "|  Baltimore|2012-01-01|United States|               2.382|\n",
      "|Baton Rouge|2012-01-01|United States|              15.038|\n",
      "+-----------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql=\"\"\"SELECT AverageTemperature AS winter FROM glt \n",
    "       WHERE dt='2012-01-01'\n",
    "       AND country = 'United States'\"\"\"\n",
    "temp=spark.sql(sql)\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+------------------+\n",
      "|       city|        dt|      country|            winter|\n",
      "+-----------+----------+-------------+------------------+\n",
      "|    Abilene|2012-07-01|United States|            29.581|\n",
      "|      Akron|2012-07-01|United States|            24.966|\n",
      "|Albuquerque|2012-07-01|United States|23.555999999999997|\n",
      "| Alexandria|2012-07-01|United States|            26.629|\n",
      "|  Allentown|2012-07-01|United States|            24.479|\n",
      "|   Amarillo|2012-07-01|United States|            28.553|\n",
      "|    Anaheim|2012-07-01|United States|            18.984|\n",
      "|  Anchorage|2012-07-01|United States|            10.697|\n",
      "|  Ann Arbor|2012-07-01|United States| 24.91500000000001|\n",
      "|    Antioch|2012-07-01|United States|            19.632|\n",
      "|  Arlington|2012-07-01|United States|            30.415|\n",
      "|  Arlington|2012-07-01|United States|            26.629|\n",
      "|     Arvada|2012-07-01|United States|            15.634|\n",
      "|    Atlanta|2012-07-01|United States|             26.34|\n",
      "|     Aurora|2012-07-01|United States|            24.301|\n",
      "|     Aurora|2012-07-01|United States|            26.982|\n",
      "|     Austin|2012-07-01|United States|            29.164|\n",
      "|Bakersfield|2012-07-01|United States|            26.095|\n",
      "|  Baltimore|2012-07-01|United States|            26.629|\n",
      "|Baton Rouge|2012-07-01|United States|            28.441|\n",
      "+-----------+----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql=\"\"\"SELECT AverageTemperature AS summer FROM glt \n",
    "       WHERE dt='2012-07-01'\n",
    "       AND country = 'United States'\"\"\"\n",
    "temp=spark.sql(sql)\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+\n",
      "|unique_city|latitude|longitude|\n",
      "+-----------+--------+---------+\n",
      "|    Abilene|  32.95N|  100.53W|\n",
      "|      Akron|  40.99N|   80.95W|\n",
      "|Albuquerque|  34.56N|  107.03W|\n",
      "| Alexandria|  39.38N|   76.99W|\n",
      "|  Allentown|  40.99N|   74.56W|\n",
      "|   Amarillo|  34.56N|  101.19W|\n",
      "|    Anaheim|  32.95N|  117.77W|\n",
      "|  Anchorage|  61.88N|  151.13W|\n",
      "|  Ann Arbor|  42.59N|   82.91W|\n",
      "|    Antioch|  37.78N|  122.03W|\n",
      "|  Arlington|  39.38N|   76.99W|\n",
      "|  Arlington|  32.95N|   96.70W|\n",
      "|     Arvada|  39.38N|  106.13W|\n",
      "|    Atlanta|  34.56N|   83.68W|\n",
      "|     Aurora|  40.99N|   87.34W|\n",
      "|     Aurora|  39.38N|  104.05W|\n",
      "|     Austin|  29.74N|   97.85W|\n",
      "|Bakersfield|  36.17N|  119.34W|\n",
      "|  Baltimore|  39.38N|   76.99W|\n",
      "|Baton Rouge|  29.74N|   90.46W|\n",
      "+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"SELECT DISTINCT city AS unique_city, latitude, longitude FROM glt\n",
    "         WHERE country = 'United States'\n",
    "         ORDER BY unique_city\"\"\"\n",
    "temp=spark.sql(sql)\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes = pd.read_csv(airport_codes_url)\n",
    "airport_codes = spark.createDataFrame(airport_codes)\n",
    "airport_codes.createOrReplaceTempView('airports')\n",
    "sql = \"\"\"SELECT ident, type, name, elevation_ft, continent, \n",
    "                iso_country, iso_region, municipality, gps_code, iata_code AS airport_code, coordinates\n",
    "         FROM airports WHERE iata_code IS NOT NULL\n",
    "         UNION\n",
    "         SELECT ident, type, name, elevation_ft, continent,\n",
    "                iso_country, iso_region, municipality, gps_code, local_code AS airport_code, coordinates\n",
    "         FROM airports WHERE local_code IS NOT NULL\"\"\"\n",
    "airports = spark.sql(sql)\n",
    "airports.createOrReplaceTempView('airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_city_demographics=pd.read_csv(us_city_demographics_url, sep=';')\n",
    "us_city_demographics['id_'] = pd.Series([str(uuid.uuid1()) for each in range(len(us_city_demographics))])\n",
    "us_city_demographics=spark.createDataFrame(us_city_demographics)\n",
    "us_city_demographics.createOrReplaceTempView('us_cities')\n",
    "sql=\"\"\"SELECT id_, city, `Median Age` AS median_age, `Male Population` AS male_population,\n",
    "              `Female Population` AS female_population, `Total Population` AS population,\n",
    "              `Number of Veterans` AS num_veterans, `Foreign-born` AS foreign_born, `Average Household Size` AS avg_household_size,\n",
    "              `State Code` AS state, race, count\n",
    "       FROM us_cities\"\"\"\n",
    "us_cities = spark.sql(sql)\n",
    "us_cities.createOrReplaceTempView('us_cities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+---------------------------+-----------+--------+---------+\n",
      "|                 id_|        dt|     avg_temperature|avg_temperature_uncertainty|       city|latitude|longitude|\n",
      "+--------------------+----------+--------------------+---------------------------+-----------+--------+---------+\n",
      "|78bdec90-f682-11e...|2012-01-01|               7.996|                      0.204|    Abilene|  32.95N|  100.53W|\n",
      "|78bded76-f682-11e...|2012-07-01|              29.581|        0.28800000000000003|    Abilene|  32.95N|  100.53W|\n",
      "|78f5f218-f682-11e...|2012-01-01|-0.34399999999999986|                       0.41|      Akron|  40.99N|   80.95W|\n",
      "|78f5f310-f682-11e...|2012-07-01|              24.966|                      0.401|      Akron|  40.99N|   80.95W|\n",
      "|7908418c-f682-11e...|2012-01-01|                1.84|                      0.484|Albuquerque|  34.56N|  107.03W|\n",
      "|7908427a-f682-11e...|2012-07-01|  23.555999999999997|                      0.335|Albuquerque|  34.56N|  107.03W|\n",
      "|79152cee-f682-11e...|2012-01-01|               2.382|                      0.383| Alexandria|  39.38N|   76.99W|\n",
      "|79152dd4-f682-11e...|2012-07-01|              26.629|                      0.503| Alexandria|  39.38N|   76.99W|\n",
      "|791e0594-f682-11e...|2012-01-01|-0.04099999999999...|                      0.493|  Allentown|  40.99N|   74.56W|\n",
      "|791e067a-f682-11e...|2012-07-01|              24.479|                      0.403|  Allentown|  40.99N|   74.56W|\n",
      "|792f9d36-f682-11e...|2012-01-01|               6.534|                      0.314|   Amarillo|  34.56N|  101.19W|\n",
      "|792f9e26-f682-11e...|2012-07-01|              28.553|                       0.24|   Amarillo|  34.56N|  101.19W|\n",
      "|79495b2c-f682-11e...|2012-01-01|              15.113|                      0.665|    Anaheim|  32.95N|  117.77W|\n",
      "|79495c12-f682-11e...|2012-07-01|              18.984|                      0.354|    Anaheim|  32.95N|  117.77W|\n",
      "|795084b0-f682-11e...|2012-01-01|             -22.628|                      0.863|  Anchorage|  61.88N|  151.13W|\n",
      "|79508598-f682-11e...|2012-07-01|              10.697|                      0.481|  Anchorage|  61.88N|  151.13W|\n",
      "|79631206-f682-11e...|2012-01-01|               -1.44|                      0.239|  Ann Arbor|  42.59N|   82.91W|\n",
      "|796312ec-f682-11e...|2012-07-01|   24.91500000000001|                      0.315|  Ann Arbor|  42.59N|   82.91W|\n",
      "|796e6a48-f682-11e...|2012-01-01|               9.993|                      0.625|    Antioch|  37.78N|  122.03W|\n",
      "|796e6b42-f682-11e...|2012-07-01|              19.632|                      0.488|    Antioch|  37.78N|  122.03W|\n",
      "+--------------------+----------+--------------------+---------------------------+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o681.showString.\n: java.lang.RuntimeException: more than one row returned by a subquery used as an expression:\nSubquery scalar-subquery#2708, [id=#1678]\n+- *(1) Project [AverageTemperature#2446 AS avg_temperature#2468]\n   +- *(1) Filter (((((isnotnull(Country#2449) AND isnotnull(dt#2445)) AND (Country#2449 = United States)) AND (dt#2445 >= 2012-01-01)) AND (dt#2445 <= 2012-12-01)) AND (dt#2445 = 2012-01-01))\n      +- *(1) Scan ExistingRDD arrow[dt#2445,AverageTemperature#2446,AverageTemperatureUncertainty#2447,City#2448,Country#2449,Latitude#2450,Longitude#2451,id_#2452]\n\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:242)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.sql.execution.SparkPlan.waitForSubqueries(SparkPlan.scala:242)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:212)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.GeneratedMethodAccessor237.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-ed45af5d9927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m        WHERE uc.city='New York'\"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o681.showString.\n: java.lang.RuntimeException: more than one row returned by a subquery used as an expression:\nSubquery scalar-subquery#2708, [id=#1678]\n+- *(1) Project [AverageTemperature#2446 AS avg_temperature#2468]\n   +- *(1) Filter (((((isnotnull(Country#2449) AND isnotnull(dt#2445)) AND (Country#2449 = United States)) AND (dt#2445 >= 2012-01-01)) AND (dt#2445 <= 2012-12-01)) AND (dt#2445 = 2012-01-01))\n      +- *(1) Scan ExistingRDD arrow[dt#2445,AverageTemperature#2446,AverageTemperatureUncertainty#2447,City#2448,Country#2449,Latitude#2450,Longitude#2451,id_#2452]\n\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:242)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.sql.execution.SparkPlan.waitForSubqueries(SparkPlan.scala:242)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:212)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.GeneratedMethodAccessor237.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "sql=\"\"\"SELECT uc.id_, uc.city, uc.median_age, uc.male_population, \n",
    "              uc.female_population, uc.num_veterans, uc.foreign_born, uc.avg_household_size,\n",
    "              uc.state, uc.race, (SELECT avg_temperature FROM temperature \n",
    "                                  WHERE dt='2012-01-01') AS winter_avg_temperature,\n",
    "                                  (SELECT avg_temperature FROM temperature\n",
    "                                  WHERE dt='2012-07-01') AS summer_avg_temperature\n",
    "       FROM temperature AS t LEFT JOIN us_cities AS uc ON t.city = uc.city \n",
    "       WHERE uc.city='New York'\"\"\"\n",
    "temp=spark.sql(sql)\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[country_code: bigint, country_name: string]\n",
      "DataFrame[mode_code: bigint, mode_name: string]\n",
      "DataFrame[port_code: string, port_name: string]\n",
      "DataFrame[us_states_code: string, us_states_name: string]\n",
      "DataFrame[visacode_code: bigint, visacode_name: string]\n"
     ]
    }
   ],
   "source": [
    "print(country)\n",
    "print(mode)\n",
    "print(port)\n",
    "print(us_states)\n",
    "print(visacode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id_: string, city: string, median_age: double, male_population: double, female_population: double, population: bigint, num_veterans: double, foreign_born: double, avg_household_size: double, state_code: string, race: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "# print(i94_df)\n",
    "# print(glt_df)\n",
    "# print(airports)\n",
    "print(us_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                city|\n",
      "+--------------------+\n",
      "|        Saint George|\n",
      "|           Worcester|\n",
      "|               Tyler|\n",
      "|         Springfield|\n",
      "|              Caguas|\n",
      "|          Charleston|\n",
      "|               Pasco|\n",
      "|              Corona|\n",
      "|               Tempe|\n",
      "|     North Las Vegas|\n",
      "|              Auburn|\n",
      "|            Palatine|\n",
      "|            Thornton|\n",
      "|Augusta-Richmond ...|\n",
      "|           Bethlehem|\n",
      "|             Phoenix|\n",
      "|            Waukegan|\n",
      "|           Hollywood|\n",
      "|           Pittsburg|\n",
      "|          Toms River|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp=spark.sql(\"SELECT DISTINCT city FROM us_cities\")\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|iso_country|iso_region|\n",
      "+-----------+----------+\n",
      "|         PG|    PG-MPL|\n",
      "|         IS|      IS-4|\n",
      "|         BZ|     BZ-SC|\n",
      "|         CA|     CA-BC|\n",
      "|         CN|     CN-13|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp=spark.sql(\"SELECT iso_country, iso_region FROM airports LIMIT 5\")\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tmorrow we start from here.\n",
    "#Connect each tables above, and modify if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "In this situation, after 7am, we can import data into a NoSQL database like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from ssl import SSLContext, PROTOCOL_TLSv1, CERT_REQUIRED\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra import ConsistencyLevel\n",
    "\n",
    "ssl_context = SSLContext(PROTOCOL_TLSv1)\n",
    "ssl_context.load_verify_locations('AmazonRootCA1.pem')\n",
    "ssl_context.verify_mode = CERT_REQUIRED\n",
    "auth_provider = PlainTextAuthProvider(username=str(config['APACHE_CASSANDRA_CREDS']['CASSANDRA_USERNAME']), password=str(config['APACHE_CASSANDRA_CREDS']['CASSANDRA_PASSWORD']))\n",
    "cluster = Cluster(['cassandra.eu-west-1.amazonaws.com'], ssl_context=ssl_context, auth_provider=auth_provider, port=9142)\n",
    "print('Patient...')\n",
    "session = cluster.connect()\n",
    "\n",
    "create_keyspace=\"\"\"CREATE KEYSPACE IF NOT EXISTS \"i94\"\n",
    "                   WITH REPLICATION={'class':'SingleRegionStrategy'}\"\"\"\n",
    "session.execute(create_keyspace)\n",
    "sleep(10)\n",
    "\n",
    "create_table=\"\"\"CREATE TABLE IF NOT EXISTS \"i94\".i94 (\n",
    "                                                      year DOUBLE,\n",
    "                                                      month DOUBLE,\n",
    "                                                      birth_country DOUBLE,\n",
    "                                                      resident_country DOUBLE,\n",
    "                                                      port TEXT,\n",
    "                                                      arrive_date DOUBLE,\n",
    "                                                      mode DOUBLE,\n",
    "                                                      state_code TEXT,\n",
    "                                                      departure_date DOUBLE,\n",
    "                                                      age DOUBLE,\n",
    "                                                      visa DOUBLE,\n",
    "                                                      date_to_db DOUBLE,\n",
    "                                                      visa_issued_dep TEXT,\n",
    "                                                      occupation TEXT,\n",
    "                                                      arrival_flag TEXT,\n",
    "                                                      depart_flag TEXT,\n",
    "                                                      update_flag TEXT,\n",
    "                                                      match_arrival_depart TEXT,\n",
    "                                                      birthyear DOUBLE,\n",
    "                                                      allowed_date TEXT,\n",
    "                                                      gender TEXT,\n",
    "                                                      ins_num TEXT,\n",
    "                                                      airline TEXT,\n",
    "                                                      admission_number DOUBLE,\n",
    "                                                      flight_no TEXT,\n",
    "                                                      visatype TEXT,\n",
    "                                                      id_ TEXT,\n",
    "                                                      PRIMARY KEY(id_)\n",
    "                ) \"\"\"\n",
    "session.execute(create_table)\n",
    "sleep(10)\n",
    "print('Table well-prepared. you can input data from dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-relational DB, its characteristics is higher elasticity, faster read & write speed and evoving data volume. In our case, we should save I94 data into non-relational DB. Because this piece of data need to make ETL process almost every minutes in real world background, and it need dynamic write and read for real-time data monitoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_sql=\"\"\"INSERT INTO \"i94\".i94 (\"cicid\",\"i94yr\",\"i94mon\",\"i94cit\",\"i94res\",\"i94port\",\"arrdate\",\"i94mode\",\"i94addr\",\"depdate\",\n",
    "                              \"i94bir\",\"i94visa\",\"count\",\"dtadfile\",\"visapost\",\"occup\",\"entdepa\",\"entdepd\",\"entdepu\",\"matflag\",\n",
    "                              \"biryear\",\"dtaddto\",\"gender\",\"insnum\",\"airline\",\"admnum\",\"fltno\",\"visatype\",\"id_\")\n",
    "                              VALUES ({0},{1},{2},{3},{4},'{5}',{6},{7},'{8}',{9},\n",
    "                                      {10},{11},{12},{13},'{14}','{15}','{16}','{17}','{18}','{19}',\n",
    "                                      {20},'{21}','{22}','{23}','{24}',{25},'{26}','{27}','{28}')\"\"\"\n",
    "\n",
    "lists=[888,1991,10,999,666,'port_test',9527,777,'addr_test',10,10,10,10,10,\"visapost\",'occup','entdepa','entdepd',\n",
    "      'entdepu','mat',1984,'dtaddto','M','insnumber','AerLingus',29,'filtnumber','H1B']\n",
    "sql=original_sql.format(lists[0],lists[1],lists[2],lists[3],lists[4],lists[5],lists[6],lists[7],lists[8],lists[9],\n",
    "                       lists[10],lists[11],lists[12],lists[13],lists[14],lists[15],lists[16],lists[17],lists[18],lists[19],\n",
    "                       lists[20],lists[21],lists[22],lists[23],lists[24],lists[25],lists[26],lists[27],uuid.uuid1())\n",
    "sql=session.prepare(sql)\n",
    "sql.consistency_level = ConsistencyLevel.LOCAL_QUORUM\n",
    "session.execute(sql)\n",
    "\n",
    "# This part is going to be used in transcript.\n",
    "\n",
    "# while True:\n",
    "#     values=input(\"Insert data. Split values by comma. If data is empty, just input comma. Enter Q for quit.\")\n",
    "#     lists=values.split(',')\n",
    "#     if len(values) < 28:\n",
    "#         print('Did you lose something?')\n",
    "#         continue\n",
    "#     elif values.lower() == 'Q':\n",
    "#         print('Quit.')\n",
    "#         break\n",
    "#     else:\n",
    "#         sql=session.format(sql)\n",
    "#         sql.consistency_level = ConsistencyLevel.LOCAL_QUORUM\n",
    "#         session.execute(sql.format(lists[0],lists[1],lists[2],lists[3],lists[4],lists[5],lists[6],lists[7],lists[8],lists[9],\n",
    "#                       lists[10],lists[11],lists[12],lists[13],lists[14],lists[15],lists[16],lists[17],lists[18],lists[19],\n",
    "#                       lists[20],lists[21],lists[22],lists[23],lists[24],lists[25],lists[26],lists[27],uuid.uuid1()))\n",
    "#         next_one=input('Done. Do you wish to continue?Y/N')\n",
    "#         if next_one.lower() == 'y':\n",
    "#             continue\n",
    "#         else:\n",
    "#             print('Thanks. Quit.')\n",
    "#             break\n",
    "\n",
    "\n",
    "temp = session.execute('SELECT * FROM i94.i94')\n",
    "df = pd.DataFrame(temp, columns=['id_','admnum','airline','arrdate','biryear','cicid','count','depdate','dtadtto','dtadfile','entdepa','entdepd','entdepu','fltno','gender','i94addr','i94bir','i94cit','i94mode','i94mon','i94port','i94res','i94visa','i94yr','insnum','matflag','occup','visapost','visatype'])\n",
    "df.to_parquet('dashboard.parquet.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the script above at 7AM everyday to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
